Comprehensive Prompts: Building a Temporal 3D Scene Graph with Robotics on M1 Mac + Thunder ComputeSTEP 0: Video Data Acquisition for Robotic Scene UnderstandingYou will need high-quality video data of tabletop scenes involving object manipulation. Ensure the video:- Includes at least 3–5 distinguishable objects (e.g., mug, book, remote, plant)- Captures hand-object interactions from a fixed camera position- Has consistent lighting and minimal occlusionUse the following data sources:1. **Ego4D**: Register and download from [https://ego4d-data.org](https://ego4d-data.org)2. **EPIC-KITCHENS (Sample Download)**:```bashpip install gdowngdown https://drive.google.com/uc?id=1CMBTV39EVCZWo5f-Z3kyoZJTo1CD_Sl1unzip Sample.zip -d data/raw_videos/```3. **Custom Data**:- Use an iPhone or DSLR with 1080p or 4K video- Optionally export LiDAR or ARKit depth (if using iPhone Pro models)- Record scenes in landscape mode at 30fpsSave all clips to `data/raw_videos/`.STEP 1: Set Up the Complete Project Skeleton on Your M1 MacCreate the following directory structure:```project/├── data/│   ├── raw_videos/│   ├── frames/│   ├── tracking/│   ├── masks/│   ├── 3d_points/│   ├── graphs/│   └── simulation/├── tracker/│   └── track_objects.py├── segmenter/│   └── run_sam.py├── reconstruction/│   ├── run_colmap.sh│   └── triangulate.py├── scene_graph/│   └── build_graph.py├── robotics/│   ├── simulation_env.py│   ├── motion_planner.py│   └── task_executor.py├── llm/│   └── interpret_scene.py├── visualization/│   └── render_graph.py├── main.py├── README.md└── requirements.txt```Use Python 3.10+ with virtualenv or conda. On M1, install PyTorch with MPS support. Heavy inference should be offloaded to Thunder Compute using SSH and synced via rsync or S3.STEP 2: Track Objects Using YOLOv8 and ByteTrackInstall YOLOv8:```bashpip install ultralytics```Install ByteTrack and dependencies:```bashgit clone https://github.com/ifzhang/ByteTrackcd ByteTrack && pip install -r requirements.txt```For each video:1. Extract frames to `data/frames/{video_id}/`2. Run YOLOv8 on each frame3. Pass detections into ByteTrack4. Save output in:```json[  {"frame": 0, "object_id": 1, "class": "mug", "bbox": [x, y, w, h]}]```Store as `data/tracking/{video_id}_tracks.json`.STEP 3: Generate Segmentation Masks Using SAMInstall Segment Anything:```bashpip install git+https://github.com/facebookresearch/segment-anything.git```Steps:1. Load each frame and bounding box from tracking2. Prompt SAM with the bounding box3. Save the binary mask as PNG and update JSON with:```json"mask_path": "data/masks/video1/object1_frame000.png"```Store masks in `data/masks/{video_id}/`.STEP 4: 3D Scene Reconstruction using COLMAP and Thunder ComputeOn Thunder compute node:1. Install COLMAP2. Upload extracted video frames3. Run the following:```bashcolmap feature_extractor --database_path database.db --image_path images/colmap exhaustive_matcher --database_path database.dbmkdir sparse && colmap mapper --database_path database.db --image_path images/ --output_path sparse/```4. Export camera intrinsics and extrinsics5. Triangulate 2D centroids of segmented objects6. Save each object’s 3D trajectory:```json{  "id": 2,  "class": "book",  "trajectory": [    {"frame": 1, "3d_position": [x, y, z]},    ...  ]}```STEP 5: Construct a Temporal 3D Scene GraphUse NetworkX to construct a graph:- Nodes = tracked objects with class, trajectory, mask path- Edges = spatial and temporal relations:  - "next_to", "on", "moved", "picked_up", "placed"Store the graph as:```json{  "nodes": [...],  "edges": [    {"source": "hand_1", "target": "mug_1", "relation": "grasped", "start_frame": 4, "end_frame": 6}  ]}```STEP 6: Use DeepSeek R1 to Interpret the Graph and PlanPrompt DeepSeek R1 with the scene graph:Task: “What happened?”Goal: “Clear the table”Provide:```json{  "scene_graph": { ... },  "instruction": "clear the table"}```Expected output:```json[  {"action": "pick", "object": "mug", "source": "table", "target": "shelf"},  ...]```STEP 7: Simulate Robot Behavior in PyBulletInstall PyBullet:```bashpip install pybullet```In `robotics/simulation_env.py`:- Initialize Panda robot and load table/objects using URDF- Place objects based on 3D scene graphIn `robotics/task_executor.py`:- For each LLM instruction:  - Compute IK to reach object  - Simulate grasp  - Move to target position  - ReleaseRecord the simulation as MP4 using PyBullet's camera and logging APIs.STEP 8: Evaluate Task CompletionAfter execution, log:- Object start and end positions- Any failed grasps- Number of actions attemptedSave:- Evaluation report `results/eval_{video_id}.json`- Video `results/simulation_{video_id}.mp4`- LLM interpretation `results/summary_{video_id}.txt`